{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class mLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embed_size, output_size):\n",
    "        super(mLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # input embedding\n",
    "        self.encoder = nn.Embedding(input_size, embed_size)\n",
    "        # lstm weights\n",
    "        self.weight_fm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_im = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_cm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_om = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_fx = nn.Linear(embed_size, hidden_size)\n",
    "        self.weight_ix = nn.Linear(embed_size, hidden_size)\n",
    "        self.weight_cx = nn.Linear(embed_size, hidden_size)\n",
    "        self.weight_ox = nn.Linear(embed_size, hidden_size)\n",
    "        # multiplicative weights\n",
    "        self.weight_mh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_mx = nn.Linear(embed_size, hidden_size)\n",
    "        # decoder\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inp, h_0, c_0):\n",
    "        # encode the input characters\n",
    "        inp = self.encoder(inp)\n",
    "        # calculate the multiplicative matrix\n",
    "        m_t = self.weight_mx(inp) * self.weight_mh(h_0)\n",
    "        # forget, input and output gates\n",
    "        f_g = torch.sigmoid(self.weight_fx(inp) + self.weight_fm(m_t))\n",
    "        i_g = torch.sigmoid(self.weight_ix(inp) + self.weight_im(m_t))\n",
    "        o_g = torch.sigmoid(self.weight_ox(inp) + self.weight_om(m_t))\n",
    "        # intermediate cell state\n",
    "        c_tilda = torch.tanh(self.weight_cx(inp) + self.weight_cm(m_t))\n",
    "        # current cell state\n",
    "        cx = f_g * c_0 + i_g * c_tilda\n",
    "        # hidden state\n",
    "        hx = o_g * torch.tanh(cx)\n",
    "\n",
    "        out = self.decoder(hx.view(1,-1))\n",
    "\n",
    "        return out, hx, cx\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h_0 = Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "        c_0 = Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "        return h_0, c_0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 84, 104, 101,  32, 111, 109, 101, 103,  97,  32,  40,  79,  41,  32,\n",
      "        115, 121, 109,  98, 111, 108,  10])\n"
     ]
    }
   ],
   "source": [
    "# Converte string para uma lista de inteiros\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "\n",
    "def generate_hidden(text, rnn):\n",
    "    hidden, cell = rnn.init_hidden()\n",
    "    text_tensor = char_tensor(text).cuda()\n",
    "    for p in range(len(text)):\n",
    "        output, hidden, cell = rnn(text_tensor[p], hidden, cell)\n",
    "    return hidden, cell\n",
    "\n",
    "def char_tensor(string):\n",
    "    string = unidecode(string)\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = ord(string[c])\n",
    "        except:\n",
    "            print(c)\n",
    "            raise\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('The omega (Î©) symbol\\n'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128 # ascii representation\n",
    "hidden_size = 2048\n",
    "rnn = mLSTM(embed_size, hidden_size, embed_size, embed_size).cuda()\n",
    "rnn.load_state_dict(torch.load(\"lstm_11.pth\"))\n",
    "rnn.eval()\n",
    "\n",
    "hidden, cell = generate_hidden(\"Adorei! Chegou bem cedo!\", rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = mLSTM(embed_size, hidden_size, embed_size, embed_size).cuda()\n",
    "        self.rnn.load_state_dict(torch.load(\"lstm_11.pth\"))\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, int(hidden_size/4))\n",
    "        self.fc3 = nn.Linear(int(hidden_size/4), int(hidden_size/2))\n",
    "        self.fc4 = nn.Linear(int(hidden_size/2), 5)\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "        \n",
    "    def forward(self, text_tensor):\n",
    "#         hidden, cell = rnn.init_hidden()\n",
    "#         for p in range(len(text_tensor)):\n",
    "#             _, hidden, cell = rnn(text_tensor[p], hidden, cell)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            hidden, cell = rnn.init_hidden()\n",
    "            for p in range(len(text_tensor)):\n",
    "                _, hidden, cell = rnn(text_tensor[p], hidden, cell)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((hidden, cell), 0).view(1, -1)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.dropout(self.fc2(x)))\n",
    "        x = F.relu(self.dropout(self.fc3(x)))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"../data/interim/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/interim/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for score, message in zip(df_train.score.values, df_train.message.values):\n",
    "    train_data.append([score, message])\n",
    "    \n",
    "test_data = dict()\n",
    "for score, message in zip(df_test.score.values, df_test.message.values):\n",
    "    try:\n",
    "        test_data[score].append([score, message])\n",
    "    except:\n",
    "        test_data[score] = [[score, message]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"net_4.pth\"))\n",
    "net = net.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "loss_metric = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, it: 1000, Loss: 7.00225830078125e-05\n",
      "Epoch: 5, it: 2000, Loss: 0.0004045950174331665\n",
      "Epoch: 5, it: 3000, Loss: 1.4162540435791016e-05\n",
      "Epoch: 5, it: 4000, Loss: 0.00016620415449142457\n",
      "Epoch: 5, it: 5000, Loss: 0.00014626002311706542\n",
      "Epoch: 5, it: 6000, Loss: 2.2648255030314128e-05\n",
      "Epoch: 5, it: 7000, Loss: 6.14546707698277e-05\n",
      "Epoch: 5, it: 8000, Loss: 1.9891321659088135e-05\n",
      "Epoch: 5, it: 9000, Loss: 0.00031109343634711374\n",
      "Epoch: 5, it: 10000, Loss: 7.381534576416016e-06\n",
      "Epoch: 5, it: 11000, Loss: 0.00014631254022771662\n",
      "Epoch: 5, it: 12000, Loss: 6.305905183156331e-05\n",
      "Epoch: 5, it: 13000, Loss: 9.58567399245042e-06\n",
      "Epoch: 5, it: 14000, Loss: 8.225134440830776e-06\n",
      "Epoch: 5, it: 15000, Loss: 0.00010729586283365886\n",
      "Epoch: 5, it: 16000, Loss: 8.829474449157714e-06\n",
      "Epoch: 5, it: 17000, Loss: 0.00010780911585863899\n",
      "Epoch: 5, it: 18000, Loss: 8.535650041368273e-06\n",
      "Epoch: 5, it: 19000, Loss: 9.10864691985281e-05\n",
      "Epoch: 5, it: 20000, Loss: 0.000100390625\n",
      "Epoch: 5, it: 21000, Loss: 2.0216669355119977e-05\n",
      "Epoch: 5, it: 22000, Loss: 1.5584219585765493e-05\n",
      "Epoch: 5, it: 23000, Loss: 3.5387039184570314e-05\n",
      "Epoch: 5, it: 24000, Loss: 7.084329922993978e-05\n",
      "Epoch: 5, it: 25000, Loss: 2.78961181640625e-06\n",
      "Epoch: 5, it: 26000, Loss: 4.433492513803335e-05\n",
      "Epoch: 5, it: 27000, Loss: 7.709291246202257e-06\n",
      "Epoch: 5, it: 28000, Loss: 7.520403180803572e-08\n",
      "Epoch: 5, it: 29000, Loss: 7.96151983326879e-06\n",
      "Epoch: 5, it: 30000, Loss: 3.4968058268229167e-10\n",
      "Epoch: 6, it: 1000, Loss: 0.0015830179452896117\n",
      "Epoch: 6, it: 2000, Loss: 5.892693996429443e-05\n",
      "Epoch: 6, it: 3000, Loss: 9.758432706197103e-05\n",
      "Epoch: 6, it: 4000, Loss: 0.00024203535914421083\n",
      "Epoch: 6, it: 5000, Loss: 0.0006092291831970214\n",
      "Epoch: 6, it: 6000, Loss: 0.0002799525459607442\n",
      "Epoch: 6, it: 7000, Loss: 0.00013511441435132707\n",
      "Epoch: 6, it: 8000, Loss: 6.532669067382813e-08\n",
      "Epoch: 6, it: 9000, Loss: 6.550984912448459e-05\n",
      "Epoch: 6, it: 10000, Loss: 8.246159553527832e-06\n",
      "Epoch: 6, it: 11000, Loss: 8.088892156427557e-06\n",
      "Epoch: 6, it: 12000, Loss: 0.00018408159414927164\n",
      "Epoch: 6, it: 13000, Loss: 0.000207000915820782\n",
      "Epoch: 6, it: 14000, Loss: 0.0001042525087084089\n",
      "Epoch: 6, it: 15000, Loss: 4.726392428080241e-05\n",
      "Epoch: 6, it: 16000, Loss: 4.435807466506958e-06\n",
      "Epoch: 6, it: 17000, Loss: 7.991103564991671e-06\n",
      "Epoch: 6, it: 18000, Loss: 0.00011753148502773709\n",
      "Epoch: 6, it: 19000, Loss: 2.6638771358289218e-05\n",
      "Epoch: 6, it: 20000, Loss: 8.047189712524414e-05\n",
      "Epoch: 6, it: 21000, Loss: 7.663990202404204e-05\n",
      "Epoch: 6, it: 22000, Loss: 2.5205801833759654e-05\n",
      "Epoch: 6, it: 23000, Loss: 8.445839259935462e-05\n",
      "Epoch: 6, it: 24000, Loss: 8.753677209218342e-05\n",
      "Epoch: 6, it: 25000, Loss: 6.334276676177978e-05\n",
      "Epoch: 6, it: 26000, Loss: 5.216112503638634e-06\n",
      "Epoch: 6, it: 27000, Loss: 4.916049815990307e-06\n",
      "Epoch: 6, it: 28000, Loss: 0.0\n",
      "Epoch: 6, it: 29000, Loss: 0.0001527584174583698\n",
      "Epoch: 6, it: 30000, Loss: 9.173000653584798e-05\n",
      "Epoch: 7, it: 1000, Loss: 0.0016094379425048828\n",
      "Epoch: 7, it: 2000, Loss: 0.00017621731758117677\n",
      "Epoch: 7, it: 3000, Loss: 1.3286431630452474e-05\n",
      "Epoch: 7, it: 4000, Loss: 1.7034292221069337e-05\n",
      "Epoch: 7, it: 5000, Loss: 7.696466445922851e-05\n",
      "Epoch: 7, it: 6000, Loss: 0.00026823965708414716\n",
      "Epoch: 7, it: 7000, Loss: 0.00022991970607212612\n",
      "Epoch: 7, it: 8000, Loss: 5.245208740234375e-09\n",
      "Epoch: 7, it: 9000, Loss: 0.00037208853827582465\n",
      "Epoch: 7, it: 10000, Loss: 0.0002002673625946045\n",
      "Epoch: 7, it: 11000, Loss: 0.00015073559500954368\n",
      "Epoch: 7, it: 12000, Loss: 4.1467547416687014e-05\n",
      "Epoch: 7, it: 13000, Loss: 2.46431644146259e-05\n",
      "Epoch: 7, it: 14000, Loss: 1.869572911943708e-05\n",
      "Epoch: 7, it: 15000, Loss: 7.989724477132161e-06\n",
      "Epoch: 7, it: 16000, Loss: 7.501700520515441e-05\n",
      "Epoch: 7, it: 17000, Loss: 1.5424868639777688e-05\n",
      "Epoch: 7, it: 18000, Loss: 2.1300488048129612e-05\n",
      "Epoch: 7, it: 19000, Loss: 0.00011744838011892218\n",
      "Epoch: 7, it: 20000, Loss: 3.7974119186401368e-06\n",
      "Epoch: 7, it: 21000, Loss: 4.235562824067616e-06\n",
      "Epoch: 7, it: 22000, Loss: 1.10502459786155e-05\n",
      "Epoch: 7, it: 23000, Loss: 3.922865701758343e-05\n",
      "Epoch: 7, it: 24000, Loss: 2.135404944419861e-05\n",
      "Epoch: 7, it: 25000, Loss: 6.437751770019532e-05\n",
      "Epoch: 7, it: 26000, Loss: 4.108135516826923e-09\n",
      "Epoch: 7, it: 27000, Loss: 4.527855802465368e-05\n",
      "Epoch: 7, it: 28000, Loss: 3.1878599098750524e-05\n",
      "Epoch: 7, it: 29000, Loss: 1.2771992847837251e-05\n",
      "Epoch: 7, it: 30000, Loss: 4.775412877400716e-06\n",
      "Epoch: 8, it: 1000, Loss: 0.00012276244163513184\n",
      "Epoch: 8, it: 2000, Loss: 0.0008047189712524414\n",
      "Epoch: 8, it: 3000, Loss: 0.00012967737515767416\n",
      "Epoch: 8, it: 4000, Loss: 0.00032667696475982663\n",
      "Epoch: 8, it: 5000, Loss: 0.00035583553314208987\n",
      "Epoch: 8, it: 6000, Loss: 0.00035350831349690757\n",
      "Epoch: 8, it: 7000, Loss: 0.0005887701851981027\n",
      "Epoch: 8, it: 8000, Loss: 5.047968029975891e-05\n",
      "Epoch: 8, it: 9000, Loss: 0.0001544862323337131\n",
      "Epoch: 8, it: 10000, Loss: 8.105766773223877e-05\n",
      "Epoch: 8, it: 11000, Loss: 0.00014837346293709495\n",
      "Epoch: 8, it: 12000, Loss: 1.6622960567474365e-05\n",
      "Epoch: 8, it: 13000, Loss: 3.547529073861929e-05\n",
      "Epoch: 8, it: 14000, Loss: 0.00011495985303606306\n",
      "Epoch: 8, it: 15000, Loss: 0.0001404458204905192\n",
      "Epoch: 8, it: 16000, Loss: 8.186459541320801e-06\n",
      "Epoch: 8, it: 17000, Loss: 5.325473056120031e-05\n",
      "Epoch: 8, it: 18000, Loss: 3.215471903483073e-07\n",
      "Epoch: 8, it: 19000, Loss: 1.4408525667692485e-05\n",
      "Epoch: 8, it: 20000, Loss: 1.0330438613891603e-06\n",
      "Epoch: 8, it: 21000, Loss: 1.1158068974812826e-05\n",
      "Epoch: 8, it: 22000, Loss: 4.6202919699928976e-06\n",
      "Epoch: 8, it: 23000, Loss: 7.799583932627803e-06\n",
      "Epoch: 8, it: 24000, Loss: 6.705991427103679e-05\n",
      "Epoch: 8, it: 25000, Loss: 2.149233818054199e-05\n",
      "Epoch: 8, it: 26000, Loss: 7.07313739336454e-05\n",
      "Epoch: 8, it: 27000, Loss: 1.2750978823061343e-08\n",
      "Epoch: 8, it: 28000, Loss: 6.544828840664456e-05\n",
      "Epoch: 8, it: 29000, Loss: 2.0879975680647225e-05\n",
      "Epoch: 8, it: 30000, Loss: 6.900461514790853e-06\n",
      "Epoch: 9, it: 1000, Loss: 0.00021439766883850098\n",
      "Epoch: 9, it: 2000, Loss: 0.00013861048221588135\n",
      "Epoch: 9, it: 3000, Loss: 0.0005364793141682943\n",
      "Epoch: 9, it: 4000, Loss: 1.7890453338623046e-05\n",
      "Epoch: 9, it: 5000, Loss: 0.0003933371067047119\n",
      "Epoch: 9, it: 6000, Loss: 0.001064714511235555\n",
      "Epoch: 9, it: 7000, Loss: 0.00022991970607212612\n",
      "Epoch: 9, it: 8000, Loss: 0.0003390270471572876\n",
      "Epoch: 9, it: 9000, Loss: 7.377730475531684e-06\n",
      "Epoch: 9, it: 10000, Loss: 3.404092788696289e-06\n",
      "Epoch: 9, it: 11000, Loss: 6.396435607563366e-05\n",
      "Epoch: 9, it: 12000, Loss: 4.881852865219116e-05\n",
      "Epoch: 9, it: 13000, Loss: 9.453309499300443e-05\n",
      "Epoch: 9, it: 14000, Loss: 6.433078220912389e-06\n",
      "Epoch: 9, it: 15000, Loss: 0.00015512259801228842\n",
      "Epoch: 9, it: 16000, Loss: 7.634282112121582e-06\n",
      "Epoch: 9, it: 17000, Loss: 0.000123032247318941\n",
      "Epoch: 9, it: 18000, Loss: 0.0001226259469985962\n",
      "Epoch: 9, it: 19000, Loss: 8.470726013183594e-05\n",
      "Epoch: 9, it: 20000, Loss: 3.808838725090027e-05\n",
      "Epoch: 9, it: 21000, Loss: 0.00013708118030003139\n",
      "Epoch: 9, it: 22000, Loss: 0.00010026985948736017\n",
      "Epoch: 9, it: 23000, Loss: 1.9959584526393725e-05\n",
      "Epoch: 9, it: 24000, Loss: 6.1776439348856605e-06\n",
      "Epoch: 9, it: 25000, Loss: 1.56390380859375e-05\n",
      "Epoch: 9, it: 26000, Loss: 7.49888328405527e-05\n",
      "Epoch: 9, it: 27000, Loss: 5.643064887435348e-05\n",
      "Epoch: 9, it: 28000, Loss: 2.058403832571847e-06\n",
      "Epoch: 9, it: 29000, Loss: 5.549786008637527e-05\n",
      "Epoch: 9, it: 30000, Loss: 5.999682346979777e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5, 10):\n",
    "    epoch_loss = 0\n",
    "    random.shuffle(train_data)\n",
    "    for i, (score, message) in enumerate(train_data):\n",
    "        net.zero_grad()\n",
    "        pred = net(char_tensor(message).cuda())\n",
    "        score = torch.LongTensor([score-1]).cuda()\n",
    "        loss = loss_metric(pred, score)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            print(\"Epoch: {}, it: {}, Loss: {}\".format(epoch, i, loss.data.item() / i))\n",
    "            \n",
    "    torch.save(net.state_dict(), \"net_{}.pth\".format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
